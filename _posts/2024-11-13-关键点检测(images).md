---
layout: post
comments: True
title: "关键点检测(images)"
date: 2024-07-30 01:09:00

---

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

---


## 2D Keypoint Detection

### Supervised 2D keypoint Detection

#### \[**ECCV 2016**\] [Stacked hourglass networks for human pose estimation](https://arxiv.org/pdf/1603.06937.pdf)

*Alejandro Newell, Kaiyu Yang, Jia Deng*

[CODE](http://www-personal.umich.edu/~alnewell/pose)

![hourglass0]({{ '/assets/images/HOURGLASS-0.PNG' | relative_url }}){: width=200px style="float:center"} 

这篇文章是2D keypoints detection的经典之一（因为它的Hourglass网络结构）。

Hourglass网络的设计受启发于获取各个尺度的信息的需求。尽管局部信息对于识别比如faces或者hands等局部物体来说很重要，但最终的pose estimation需要对整个human body有一个全面的理解。人的orientations，四肢的arrangements，以及相邻关节的关系等等，这些信息都需要同一张图片不同尺度的信息才能良好的获得。Hourglass网络能够将各个尺度的信息综合起来从而输出pixel-wise的预测。
这样的网络必须要有某些机制来有效的处理和合并不同尺度的特征。之前有些工作使用独立的不同的网络来处理不同分辨率的图片输入，再将这些输入综合起来处理。但Hourglass network使用的是一个单一的pipeline，使用skip connections来在各个分辨率下保存空间信息。

Hourglass网络的结构如下：卷积层和max pooling层将输入图片的分辨率降低到一个很低的值。在每个max pooling层，在做max pooling之前，输入分叉为两部分，不进入max pooling的那部分会做更多的卷积操作，留着之后使用。最后在达到了最低分辨率之后，网络开始进行upsampling以及结合之前不同分辨率下的features的操作。为了将两个相邻分辨率的特征结合起来，作者对较低分辨率的feature map使用了最近邻upsampling方法，再加上较高分辨率的那个feature map，从而得到输出。hourglass网络的结构是对称的，之前每用max pooling降低分辨率一次，之后就使用upsampling提升分辨率一次，所以最终的分辨率会和输入图片分辨率一样。在之后，再加上两个卷积大小为$$1 \times 1$$的卷积层，从而得到网络最终的输出。网络的最终输出是一系列的heatmaps，每个heatmap表示的是每个keypoint在图片中每个像素点出现的概率值的大小。

初始输入的图片分辨率为$$256 \times 256$$，为了节约计算成本，作者在将图片输入hourglass模块之前，先通过了一个带有残差链接的$$stride=2$$，$$padding=3$$，卷积核为7的卷积层，和一个max pooling层，从而分辨率变为$$64 \times 64$$，之后在进入上述所说的hourglass模块里。对于hourglass里所有层输出的feature maps，其通道数都是256。

本文提出的最终的网络结果是多个hourglass模块堆叠而成，前一个hourglass模块的输出是后一个hourglass模块的输入。这样的结构使得网络可以重新衡量初始的估计值，改进效果。而且使用这种结构，网络中间某个hourglass模块的输入也可以拿来计算loss，具体做法是，对于中间某个hourglass模块的输出，其经过$$1 \times 1$$卷积得到heatmap输出，从而可以拿来被计算loss，而这个heatmaps再经过$$1 \times 1$$的卷积层回到操作之前的通道数，再和这个hourglass的输出加起来，作为下一个hourglass模块的输入。每个hourglass模块的权重并不共享，而且每个hourglass拿来计算的loss使用的是同一种loss以及同一个ground truth。

本文的evaluation是根据标准的Percentage of Correct Keypoints（PCK）metric来计算的，给定一个normalized的distance，落在ground truth这个distance以内的都算判断正确，而PCK metric计算的是判断正确的keypoints占的比例。本文使用了FLIC和MPII两个数据集来测试效果，对于FLIC数据集，这个distance是利用躯干的大小进行了normalize，而对于MPII数据集，这个distance是根据头的大小进行了normalize。

> 这篇文章并不能解决多人的问题，对于多人的情况，只会考虑最靠近图片中心的那个。如果要看多人的算法，可以去看OpenPose那篇论文。

> 本文的方法不能检测到被遮挡住的keypoints，这仍然是一个需要被解决的问题。


#### \[**CVPR 2017** $$\&$$  **TPAMI 2019**\] [OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields](https://arxiv.org/pdf/1812.08008.pdf)
*Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh*

[post](https://github.com/CMU-Perceptual-Computing-Lab/openpose)

OpenPose是一个成熟的可以进行multi-person的2D关键点检测的算法（已商业化）

对于单人的pose estimation有很多论文已经做得很好了，但对于多人来说，有以下几个困难的地方：1）首先，我们并不知道图里到底有几个人，而且每个人所在的位置、大小都不清楚；2）其次，人与人之间可能存在干涉，比如说遮挡、关节的旋转等等，很难分清楚到底哪部分属于哪一个人；3）以往的论文里的算法的复杂度都会随着图里人数量的增加而增加，从而很难实现实时。

对于multi-person 2D pose estimation，top-down的方式很常见，也就是先检测图片里有几个人，然后对每个人实行pose estimation，因为单人的pose estimation已经做得很好了，这个算法并不复杂。但它存在着两个很大的问题：首先如果一开始检测人的时候就检测错了或者遗漏了，那之后是没有补救办法的；其次，这样的方法需要对检测出来的每个人都做单人pose estimation，这会使得算法的复杂度和人的数量成正比。所以说，bottom-up的方法也被提了出来，这种方法有能够解决上述两个问题的潜力。但之前的bottom-up方法仍然效率不高，因为它们在最后还是需要利用全局信息来辅助判断，从而要花不少时间。

这篇文章里利用Part Affinity Field实现了实时的multi-person 2D pose estimation。Part Affinity Field是一个2D向量的集合，表示的是四肢的位置和方向信息。利用bottom-up的方式，将detection和association结合起来（模型有两个主要部分，一个是PAF refinement，另一个是body part prediction refinement，而PAF就是association）逐步推进，可以在利用很小的计算资源的情况下达到很好的效果。

![overview]({{ '/assets/images/OPENPOSE-1.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 1. 算法流程。输入是一张大小为$$w \times h$$的RGB图片，而输出为图中每个人的生理结构上的2D keypoints的位置。首先，一个feedforward网络输出一个集合$$S = (S_1, S_2, ..., S_J)$$有J个confidence maps，每个对应一个身体部位，其中$$S_j \in R^{w \times h}$$，$$j = \lbrace 1,2,...,J \rbrace$$，用来表示各个身体部位位置的2D confidence maps，和一个part affinity fields（也就是2D的向量）的集合$$L= (L_1, L_2, ..., L_C)$$，每个$$L_c \in R^{w \times h \times 2}$$对应一个肢体，$$c \in \lbrace 1,...,C \rbrace$$，用来表示各个身体部位之间的从属程度。最终，confidence maps和PAFs（也就是集合S和集合L）通过greedy inference联系了起来用于输出图中所有人的2D keypoints位置。*

![architecture]({{ '/assets/images/OPENPOSE-3.PNG' | relative_url }}){: width=400px style="float:center"}
*Fig 3. OpenPose里的stages。上图左边部分用来预测PAFs $$L^t$$, 而右侧部分用来预测confidence maps $$S^t$$。每个stage的输出和图片feature连接起来，再作为下一个stage的输入。*

> 注意每个stage的CNN并不共享参数

> OpenPose有两个版本：TPAMI版本和CVPR版本，在CVPR版本里，每个stage都会同时输出PAF和confidence maps，然后结合图片features作为下个stage的输入。而在TPAMI版本里，前$$T_P$$个stages仅仅输出PAF，而后$$T_C$$个stages仅输出confidence map，这样可以大大减少计算量。这样的改进是因为作者通过实验注意到，减少每个stage的输出，并不会太多影响实验结果，而且需要将PAF的stages放在前面。这也符合intuition，因为如果给定一张图片的PAF，很容易猜到每个keypoint在哪里，但如果只是给一系列的keypoints，不仅无法知道单个人的keypoints怎么连接，更不知道keypoints都分别是属于哪个人的。

对于PAF的stage $$t_i$$和confidence map的stage $$t_k$$的loss function分别是：

$$ \mathcal{L}_{P}^{t_i} = \sum_{c=1}^C \sum_p W(p) \lvert L_c^{t_i}(p) - L_c^{\ast}(p) \rvert^2_2, \quad \mathcal{L}_{C}^{t_k} = \sum_{j=1}^J \sum_p W(p) \lvert S_j^{t_k}(p) - S_j^{\ast}(p) \rvert^2_2 $$

其中$$L_c^{\ast}$$是第$$c$$个肢体的PAF的ground truth，$$S_j^{\ast}$$是第$$j$$个keypoint的confidence map的ground truth，上述两个求和都是对所有像素进行的，$$p$$指的就是像素点。$$W(p)$$是图片的mask。

> 在每个stage都使用loss function，用来解决梯度消失的问题，因为每个stage结尾都有loss function，对梯度的值进行了补充。

网络整体的loss function就是$$ \mathcal{L} = \sum_{t=1}^{T_P} \mathcal{L}_{P}^{t} + \sum_{t=T_P + 1}^{T_P + T_C} \mathcal{L}_{C}^t $$

Confidence maps GT，$$S_j^{\ast}$$的计算方式是，先对于图片里的每个人$$k$$生成confidence map，$$S_{j,k}^{\ast}$$。$$x_{j,k} \in \mathbb{R}^2$$表示人$$k$$的keypoint$$j$$的ground truth。从而$$ S_{j,k}^{\ast}(p) = exp(-\lvert p-x_{j,k} \rvert^2_2 / \sigma^2) $$，其中$$\sigma$$控制峰的大小。从而整个图片（可能包含多个人）的ground truth就是：$$ S_j^{\ast}(p) = \max\limits_{k} S_{j,k}^{\ast}(p) $$

Part Affinity Fieds (PAFs) GT的计算方式较为复杂一点。每一个PAF都是一个2D的vector field。对于每个肢体的PAF的每个像素位置的值，其都是一个2D的向量，包含了位置和这个肢体一个keypoint指向另一个keypoint的方向。预先定义好的body part keypoints对决定了哪些有哪些keypoints对构成肢体。

对于某张图片，其可能含有多个人，假设$$x_{j_1, k}$$和$$x_{j_2, k}$$是人$$k$$的两个keypoints，这keypoints对（有序）组成了肢体$$c$$。对于肢体$$c$$上的一点$$p$$，$$L_{c,k}^{\ast}(p)$$是一个单位向量，从$$j_1$$指向$$j_2$$，对于其它的点，$$L_{c,k}^{\ast}(p)$$的值都是0。但肢体不仅是一条直线，而是一个具有一定宽度的长方形。具体来说：$$v = (x_{j_2, k} - x_{j_1, l}) / \lvert x_{j_2, k} - x_{j_1, l} \rvert$$是从$$x_{j_1, k}$$指向$$x_{j_2, k}$$的单位向量，$$v_{\bot}$$是垂直于$$v$$的一个单位向量，而对于任意的像素点$$p$$，如果其满足$$ 0 \leq v (p - x_{j_1,k}) \leq \lVert x_{j_2, k} - x_{j_1, k} \rVert$$ 和 $$\lvert v_{\perp} (p - x_{j_1,k}) \rvert \leq \sigma_l$$，那么就认为$$p$$在肢体$$c$$上，从而$$L_{c,k}^{\ast}(p) = v$$，别的像素点位置的值就是0（长度为2的0向量）。

而整个图片的PAF的ground truth是对于所有人取了均值：$$ L_c^{\ast}(p) = 1/n_c(p) \Sigma_k L_{c,k}^{\ast}(p) $$，其中$$n_c(p)$$统计的是在点$$p$$处非零向量的PAF的个数，也就是对应着有相交的肢体的情况。

有了上述confidence map和PAF的GT的定义，就可以开始网络的训练了。

而在测试过程中，对于输入的图片，对于每个keypoint，能得到一个预测的confidence map，使用non-maximum suppresion（非极大抑制）可以得到该keypoints可能出现的多个位置，这些多个位置可能对应着图片里多个人的该keypoint，但也可能是假阳性。为了进一步筛选，就需要使用预测的PAF。对于每张图片，我们得到了$$C$$个PAF，对于每个PAF，我们计算连接该PAF对应的两个keypoints之间的线段上PAF的积分来作为score，用来衡量这两个keypoints是否构成了一个肢体。

> 如果这两个keypoints按照之前的non-maximum suppression的计算，都有好几个预测值，那么对于每对都需要计算该积分

这个积分具体是这样算的，对于两个检测到的keypoints，$$d_i$$和$$d_j$$：

$$ E_{d_i, d_j} = \int_{u=0}^{u=1} L_c(p(u)) (d_j - d_i)/ \lVert d_j - d_i \rVert du, \quad p(u) = (1-u) d_i + u d_j$$

> 在具体操作的时候，上述积分使用离散点的和来近似

从而我们将从一系列所有keypoints的candidates里挑选最终的keypoint set的问题描述为了一个最大化所有的PAF对应的score的和的优化问题，这个优化问题的约束条件是，每个PAF必须包含且仅包含该PAF对应的两个keypoints的分别各一个candidate。然而这个优化问题是个NP-hard的问题。这篇文章使用一种greedy relaxation的方法，持续性的产生高质量的匹配。

> 文章猜测这种方法有效的原因是上述计算的积分值（也就是每个肢体的score）潜在的含有global信息，因为PAF的框架具有较大的感受野

具体来说，首先，我们获得整张图片所有keypoints的所有candidates的集合，$$D_J = \lbrace d_{j}^{m}: j = 1, ..., J, m = 1, ..., N_j \rbrace$$，$$N_j$$是keypoint $$j$$的candidates的数量，而$$d_j^m \in R^2$$是keypoint $$j$$的第$$m$$个candidate的position。定义$$z_{i,j}^{m,n} \in \{0, 1\}$$来表示两个keypoint $$i,j$$的两个候选$$d_{i}^m$$和$$d_{j}^n$$是否构成肢体。肢体的候选集合为$$Z = \{z_{i, j}^{m,n}: i, j \in \{1, ..., J\}, m \in \{1, ..., N_i \}, n \in \{1, ..., N_j \}$$。

严格来描述这个优化问题，对于一对keypoint对$$i$$和$$j$$（比如说neck和right-hip），以及它们组成的肢体$$c$$，目标是：

$$ \max\limits_{Z_c} E_c = \max\limits_{Z_c} \sum_{m \in D_i} \sum_{n \in D_j} E_{m,n} z_{i, j}^{m,n}, \quad s.t., \forall m \in D_i, \sum_{n \in D_j} z_{i, j}^{m, n} \leq 1, \quad \text{and} \quad \forall n \in D_j, \sum_{m \in D_i} z_{i, j}^{m,n} \leq 1$$

> 上述定义的constraint，使得我们所优化到的结果里不会有两个$$c$$肢体公用同一个keypoint。而对于该优化目标$$E_c$$，我们可以用Hungarian算法来获取上述优化的结果。

但问题的目标$$Z$$是所有的肢体的目标$$Z_c$$的和，计算$$Z$$的最优值是一个$$K-Matching$$问题（$$K$$是肢体的数量），这个问题是个NP-hard的。对于该问题，有很多relaxations的算法存在。在这篇论文中，因为按照对人体的肢体的定义，所有的肢体$$c$$以及所有的keypoints构成的集合是一个spanning tree，作者将上述$$K-Matching$$问题分解为一系列二分匹配的子问题并且独立的解决这些问题，也就是将$$Z$$的优化分解成了独立对每个$$Z_c$$进行优化，即：

$$ \max\limits_{Z} E = \sum_{c=1}^C \max\limits_{Z_c} E_c, \quad s.t., \forall c, \quad \forall m \in D_i, \sum_{n \in D_j} z_{i, j}^{m, n} \leq 1, \quad \text{and} \quad \forall n \in D_j, \sum_{m \in D_i} z_{i, j}^{m,n} \leq 1 $$

> 第二个relaxation之所以可行，直觉上来说，spanning tree里相邻的两个node之间的关系是由预测PAF的网络学习到的，而非相邻的两个node之间的关系是由预测Keypoint的网络学习到的。

从而我们将这个优化问题分解为独立的每个pair的优化问题，而这个在之前所述，可以用Hungarian算法解决。我们再将有共同keypoint的肢体联合起来，这样其就表示出了一个完整的人的pose，或者说骨架。


### Unsupervised 2D Keypoint Detection

#### \[**ICCV 2017**\] [Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)

[CODE](https://github.com/alldbi/Factorized-Spatial-Embeddings)

这篇文章的方法基于factorizing image deformations，这种deformation可能是由viewpoint变化引起的，或者是因为物体本身的形变引起的。作者提出的方法基于的假设是deformation前后的图片的keypoints是consistent的。

> 实际上，通过对图片加上人为构造的deformation，将原始图片上所检测到的keypoint加上deformation，以及加上deformation之后的图片所检测到的keypoint，的差异作为约束条件，是现如今非常常见的一种约束keypoint detection的辅助loss，在FewShot3DKP，StableKeypoint里都有这样的loss

$$S \subset \mathbb R^3$$表示一个物体的3维surface，比如说一只鸟，$$\pmb x: \Lambda \rightarrow \{0,1,\cdots,255 \}^3$$表示的是这个物体的一张RGB图片，其中$$\Lambda$$是image grid。$$S$$是3维物体的形状，其和$$\pmb x$$是无关的。作者考虑的是学习一个函数$$q = \Phi_S(p; \pmb x)$$将$$S$$上的点$$p \in S$$映射到对应的图片$$\pmb x$$上的某个点$$q \in \Lambda$$上去。作者提出了一个利用viewpoint factorization来自动学习$$\Phi_S$$的方法。为了实现这个目标，我们考虑从另一个角度来看这个物体从而获得的另一张图片$$\pmb{x^{'}}$$。利用$$g$$来表示由于视角变化导致的图片歪曲，也就是$$\pmb{x^{'}} \approx \pmb x \circ g$$。利用$$\Phi_S$$，我们可以将$$g$$$分解为：

$$g = \Phi_S(\dot; \pmb{x^{'}}) \circ \Phi_S(\dot; \pmb x)^{-1}$$

等价于：$$\forall q \in S: \Phi_S(p; \pmb x \circ g) = g(\Phi_S(p; \pmb x))$$，这个约束条件表明$$S$$上所检测到的点$$p$$需要随着视角的变化仍然保持一致。

> 上述的$$g$$可以用来描述由于视角、物体本身的deformation以及category-specific image collection里不同个体的shape variance

为了学习这个函数$$\Phi_S$$，作者将这个函数利用一个深度神经网络来表示，并采用一种Siamese的结构，输入是$$(\pmb x, \pmb{x^{'}}, g)$$

> 如果我们任意给定一个物体的两个视角的图片，那么$$g$$一般是不知道的，所以说与其再利用某种方法来估计$$g$$，不如直接随机的构造出$$g$$，再利用$$g$$从$$\pmb x$$上构造出$$\pmb{x^{'}}$$。

$$\Phi_S$$是由神经网络所表示的，可以使用现有的任何一种用于keypoint detection的框架来实现它（比如HRnet，Hourglass等）。网络的输入是图片$$x$$，输出是检测到的keypoints，主要的loss如下：

$$\mathcal L_{align} = \frac{1}{K} \sum_{r=1}^K \lVert \Phi(\pmb x_r \circ g) - g(\Phi(\pmb x_r)) \rVert^2$$

上述的loss会使得网络学习到的keypoints都是consistent的，但并没有阻止网络所学习到的都是同一个点，也就是这些点全都聚集到了一起。为了避免这个现象，作者加了一个diversity loss来使得这些probability maps对应到图片的不同区域。最直接的方式就是直接对两个keypoints的heatmap的相互重合的区域增加惩罚：

$$\mathcal L_{div}(\pmb x) = \frac{1}{K^2} \sum_{r=1} \sum_{r^{'}=1} \sum_u p(u \vert \pmb x, r) p(u \vert \pmb x, r^{'})$$

上述公式的一个缺点就是计算量是quadratic的。一个简便的方法是：

$$\mathcal L_{div}^{'} (\pmb x) = \sum_u (\sum_{r=1}^K p(u \vert \pmb x, r) - \max\limits_{r=1, \cdots, K} p(u \vert \pmb x, r))$$

具体实现的时候，对于一张图片$$\pmb x$$和用thin-plate splines（薄板样条插值）TPS实现的transformation $$g_1, g_2$$，网络的输入图片对为：$$g_1 \circ \pmb x$$和$$g_2 \circ (g_1 \circ \pmb x)$$。

> 该文章最大的问题在于，所描述的keypoints在经过deformation之后仍要保持一致，这只能对于很小的deformation来说，因为大的deformation会直接导致keypoints可能不再可见。而该文章最大的贡献在于，如同前面所说，其提出的keypoints在图片经过小的deformation之后仍需要保持consistency这个性质，仍然被很多unsupervised的keypoint甚至structure检测，或者part segmentation论文当作辅助loss来使用。


#### \[**CVPR 2018**\] [Unsupervised Discovery of Object Landmarks as Structural Representataions](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhang_Unsupervised_Discovery_of_CVPR_2018_paper.pdf)

*Yuting Zhang, Yijie Guo, Yinxin Jin, Yijun Luo, Zhiyuan He, Honglak Lee*

[POST](https://www.ytzhang.net/projects/lmdis-rep/)

本文提出了一个autoencoder的框架来将检测到的landmarks显式的表示为structural representations。encoder输出的是landmarks，其加了限制从而能够输出有效的landmarks信息；而decoder模块以这些landmarks作为输入重构输入图片，从而形成一种end-to-end的结构来学习这些landmarks。

[Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)提出了一个无监督学习的方式在突破可能有transformations的情况下来检测稳定的landmarks表示物体的局部语义信息。然而，这个方法并没有显式的鼓励这些landmarks出现在那些可以用作image modeling的位置上。

![usr1]({{ '/assets/images/USR-1.PNG' | relative_url }}){: width=400px style="float:center"}

作者使用一个深度神经网络来将输入图片$$\pmb I$$转换为输出是$$K+1$$个通道的detection confidence map $$\pmb D \in \left[0,1\right]^{W \times H \times (K+1)}$$。这个confidence map会检测$$K$$个landmarks，而第$$K+1$$个通道表示的是背景。$$\pmb D$$的分辨率$$W \times H$$可以和输入图片的分辨率一样，或者更小，但是它们需要是同比例的。

作者提出一个轻量化的hourglass网络，来从输入图片中获取原始的detection score map $$\pmb R$$：

$$\pmb R = hourglass_l (\pmb I; \theta_l) \in \mathbb R^{W \times H \times (K+1)}$$

其中$$\theta_l$$表示的是网络参数。hourglass网络可以让detectors兼顾检测局部特征和利用全局信息。在获得了$$\pmb R$$之后，再在$$\pmb R$$的每个位置都做一次softmax，也就是每个位置沿着所有的通道数做一次softmax（包括表示背景的score map），从而所得到的$$\pmb D$$的每个位置，沿着所有通道的值加起来就是1，$$\pmb D$$里的每个值都在0到1之间：

$$\pmb D_k(u,v) = \frac{exp(\pmb R_k(u,v))}{\sum_{k^{'}=1}^{K+1} exp(\pmb R_{k^{'}}(u,v))}$$

其中$$\pmb D_k$$是$$\pmb D$$的第$$k$$个通道。

将$$\pmb D_k$$看作一个加权的map，作者使用加权的均值坐标作为第$$k$$个landmark的坐标，也就是：

$$(x_k, y_k) = \frac{1}{\zeta_k} \sum_{v=1}^H \sum_{u=1}^W (u,v) \cdot \pmb D_k(u,v)$$

其中$$\zeta_k = \sum_{v=1}^H \sum_{u=1}^W \cdot \pmb D_k(u,v)$$是空间归一化因子。将landmark和landmark detector简记为：

$$\pmb l = \left[x_1, y_1, \cdots, x_K, y_K \right]^T = \text{landmark}(\pmb I; \theta_l)$$

流程图上半部分（蓝色）的左半部分表示的就是landmark detector。

$$\pmb l$$里的元素应该是所检测到的landmark coordinates，但是到目前为止，并没有限制条件使得它们是landmarks，到现在为止它们只是任意的latent representations。因此，作者提出了下列soft constraints作为正则项来迫使这些所检测到的representations具有landmarks的特性。

* Concentration constraint

作为一个单一location的detection confidence map，$$\pmb D_k$$的值需要集中于一个局部的区域内。$$\pmb D_k / \zeta_k$$可以被认为是一个2维的概率分布，从而可以计算出其沿着$$x$$轴和$$y$$轴的方差，$$\sigma_{det, u}^2, \sigma_{det, v}^2$$。作者定义了如下的constraint loss来鼓励这两个方差都很小：

$$L_{conc} = 2\pi e (\sigma_{det, u}^2 + \sigma_{det, v}^2)^2$$

上述的表示的是以$$(x_k,y_k)$$为中心，以$$(\sigma_{det,u}^2 + \sigma_{det, v}^2)/2 \cdot \mathbb I$$为协方差矩阵的二维高斯分布的熵的exponential。这个高斯分布是$$\pmb D_k / \zeta_k$$的一个近似，$$L_{conc}$$越小，分布就越集中于中心点的位置，越符合要求。

该高斯分布记为：$$\overline{ \pmb{D}_k}(u,v) = (1 / WH) \mathcal N((u,v); (x_k, y_k), \sigma_{det} \mathbb I)$$

* Separation constraint

理想情况下，autoencoder的目标函数可以自动使得$$K$$个landmarks分布在不同的区域内从而很好地完成decoder里的reconstruction任务。然而，由于随机的初始化，landmarks可能互相靠得很近。这会导致优化过程陷入局部最优点。为了解决这个问题，作者提出了一个显式的loss来空间分隔这些landmarks：

$$L_{sep} = \sum_{k \neq k^{'}, k=1}^{K} exp(\frac{-\lVert (x_{k^{'}}, y_{k^{'}}) - (x_k, y_k) \rVert_2^2}{2 \sigma_{sep}^2})$$

* Equivariance constraint

图片的某一个特定的landmark应该位于一个具有明显局部特征的地方（这个局部特征也应该有明确的语义信息）。这需要landmarks对于image transformations来说具有equivariance的特性。更具体来说，如果相对应的视觉信息仍然存在于transformed之后的图片中的话，一个landmark应该要跟着transformation变化而变化（比如说camera或者object的移动）。$$g$$用来表示这种transformation，从而原image $$\pmb I$$和transformed之后的image $$\pmb I^{'}$$就有着如下的对应关系：$$\pmb I^{'}(u,v) = \pmb I(g(u,v))$$，也就是说，$$g(x_k^{'}, y_k^{'}) = (x_k ,y_k)$$，从而引入如下的限制：

$$L_{eqv} = \sum_{k=1}^K \lvert g(x_k^{'}, y_k^{'}) - (x_k, y_k) \rvert_2^2 $$

如果$$g$$是已知的话，那么这个loss就是定义好了的。和[Unsupervised learning of object landmarks by factorized spatial embeddings](https://openaccess.thecvf.com/content_ICCV_2017/papers/Thewlis_Unsupervised_Learning_of_ICCV_2017_paper.pdf)一样，作者使用一个thin plate spline（TPS）来使用随机参数模拟$$g$$。作者使用随机的translation，rotation以及scaling来为TPS确定global affine component；之后再perturb一系列control points来确定TPS的local component。除此之外，如果训练集合是以视频的形式出现的，可以将dense motion flow用作$$g$$，而下一帧就是$$\pmb I^{'}$$。

除了keypoint detector之外，这篇文章的另一个贡献是对于每个keypoint，还align了一个local feature descriptor。

对于简单的图片，比如说MINIST，landmarks就足够描述物体的形状了。但对于大多数实际的图片来说，landmarks并不足以表示所有的视觉内容，所以就需要额外的latent representations来encode补充信息。而一方面，我们不能引入过多的全局信息，因为这样会导致模型不容易学习到landmark的局部信息（因为这样的全局信息就足够decoder完成reconstruction任务了）；而另一方面，我们也需要一定的全局信息来帮助landmarks的定位。为了解决这个trade-off，作者给每个landmark都计算了一个low-dimensional的local descriptor。

作者使用另一个hourglass网络来获取一个feature map $$\pmb F$$，其和detection confidence map $$\pmb D$$的尺寸是一样的：

$$\pmb F = \text{hourglass}_f(\pmb I; \theta_f) \in \mathbb R^{W \times H \times S}$$

对于每个landmark来说，作者使用一个average pooling来获取这个landmark的local feature descriptor，其中这个average pooling的权重是由一个中心在这个landmark点的soft mask构成的。具体来说，我们将之前所说的中心点位于$$\left[ x_k, y_k \right]$$的二维高斯分布$$\overline{ \pmb{ D}_k}$$作为这个soft mask。一个可学习的linear operator（实际上就是几层MLP）被加在每个feature上从而为每个landmark学习到一个独立的feature descriptor（注意到每个landmark都有一个自己的linear operator，不是共享的）。

> 作者认为之前所学习到的feature $$\pmb F$$使得所有keypoint的features都公用一个空间，而此处对于每个landmark使用不同的linear projection将其映射到自己独立的空间里。每个landmark独有的这个linear operator使得每个landmark descriptor可以编码独有的信息

具体来说，第$$k$$个landmark的latent descriptor就是：

$$\pmb f_k = \pmb W_k \sum_{v=1}^H \sum_{u=1}^W (\overline{ \pmb{ D}_k}(u,v) \cdot \pmb F(u,v)) \in \mathbb R^{C}, \quad \text{with} \quad C < S$$

我们还需要获取一个背景的descriptor。但使用上述方法利用一个高斯分布来近似背景的confidence map是不合理的，所以直接令$$\overline{ \pmb{ D}_{K+1}} = \pmb D_{K+1} / \zeta_{K+1}$$。

将所有的landmarks的descriptors和背景的descriptor放在一起，我们就有了$$\pmb f = \left[ \pmb f_1, \pmb f_2, \cdots, \pmb f_{K+1} \right] \in \mathbb R^{C \times (K+1)}$$。流程图的下半部分的左边部分表示了获取这个landmark descriptors $$\pmb f$$的过程。

最后介绍detector的设计。

作者先从所检测到的landmark coordinates恢复detection confidence map $$\tilde{ \pmb{ D}} \in \mathbb R^{W \times H \times (K+1)}$$。具体来说，作者使用一个二维高斯分布来表示每个landmark的confidence map，其中心点是每个landmark的coordinates，其协方差矩阵是$$\sigma_{det}^2 \mathbb I$$，其中$$\sigma_{dec}$$不是之前计算出来的，是一个超参数。

$$\tilde{ \pmb{R}_k}(u,v) = \mathcal N((u,v); (x_k, y_k), \sigma_{dec}^2 \mathbb I), k=1,2,\cdots,K, \tilde{ \pmb{ R_{K+1}}} = \pmb 1$$

> $$\tilde{ \pmb{R}_k}(u,v)$$和$$\overline{ \pmb{ D}_k}(u,v)$$的区别就在于前者的协方差是固定的，后者的协方差是由encoder的heatmap算出来的。而且decoder里对background的描述也和descriptor里的不一样。

对于计算好的$$\tilde{ \pmb{R}_k}$$中，对于每个像素点位置，沿着所有的通道做一次归一化，从而得到detection score map $$\tilde{ \pmb{D}}$$：

$$\tilde{ \pmb{D}_k}(u,v) = \tilde{ \pmb{R}_k}(u,v) / \sum_{k=1}^{K+1} \tilde{ \pmb{ R}_k}(u,v)$$

流程图的上半部分的右半部分描述了这个过程。

对于之前feature descriptor network得到的每个landmark的descriptor $$f_k$$（也包括背景的），再利用一个landmark-specific linear operator $$\tilde W_k$$将每个descriptor $$f_k$$再次进行线性映射（其后再接上一个activation function，文中使用的是LeakyReLU），最后对于这些映射后的features，再次使用$$\tilde{ \pmb{ D}_k}(u,v)$$将它们unpooling成一个feature descriptor $$\tilde {\pmb {F}}$$：

$$\tilde {\pmb {F}}(u,v) = \sum_{k=1}^{K+1} \tilde{ \pmb{ D}_k}(u,v) \cdot \tau(\tilde{ \pmb{ W}_k} \pmb f_k) \in \mathbb R^{W \times H \times S}$$

其中$$\tau$$就是activation function。这个过程由流程图下半部分的右半部分所描述。

每个landmark对应的高斯分布的方差$$\sigma_{dec}^2$$决定其周围的邻居可以给这个landmark贡献多少信息。在训练一开始的时候，需要比较大的$$\sigma_{dec}^2$$来使得训练可行，也就是说每个landmark要依靠的邻居点还很多，而随着训练的进行，需要更准确的定位，也就是需要比较小的方差。对于这样两个相互矛盾的需求，作者使用不同的$$\sigma_{dec}$$来获取多个版本的$$\tilde{ \pmb{D}}$$和对应的$$\tilde{\pmb{F}}$$：$$(\tilde{ \pmb{D}_1}, \tilde{\pmb{ F}_1}), (\tilde{ \pmb{ D}_2}, \tilde{ \pmb{ F}_2}), \cdots, (\tilde{ \pmb{ D}_M}, \tilde{ \pmb{ D}_M})$$。

最后，将上述这些$$(\tilde{ \pmb{ D}_1}, \tilde{ \pmb{ F}_1}), (\tilde{ \pmb{ D}_3}, \tilde{ \pmb{ F}_2}), \cdots, (\tilde{ \pmb{ D}_M}, \tilde{ \pmb{ D}_M})$$沿着通道这个维度连起来，再输入一个hourglass网络里，最终获取重建的图片：

$$\tilde{ \pmb{ I}} = \text{hourglass}_d (\left[\tilde{ \pmb{ D}_1}, \tilde{ \pmb{ F}_1}), (\tilde{ \pmb{ D}_2}, \tilde{ \pmb{ F}_2}), \cdots, (\tilde{ \pmb{ D}_M}, \tilde{ \pmb{ D}_M} \right]; \theta_d)$$

流程图里的最右边的灰色线表示了这个过程。

图片reconstruction loss驱动了整个training的进行。整个网络的loss $$L_{AE}$$定义为：

$$L_{AE} = \lambda_{recon} L_{recon} + \lambda_{conc} L_{conc} + \lambda_{sep} L_{sep} + \lambda_{eqv} L_{eqv}$$


#### \[**NeurIPS 2018**\] [Unsupervised Learning of Object Landmarks through Conditional Image Generation](https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html)

[PAGE](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_landmarks/)

该方法的思想是，对于一对图片输入，$$\pmb{x}, \pmb{x}^{'}$$（其可能是一个视频的两帧，或者是synthetic object的deform前后的两张图片），可能具有appearance、deformation以及viewpoint的变化（但是是同一个object的两张图片），利用从$$\pmb{x}^{'}$$获取的structure信息（显式的由keypoints表示）以及从$$\pmb{x}$$获取的appearance信息，如果一个decoder能够reconstruct $$\pmb{x}^{'}$$，那么就说明我们获取的structure信息是足够好的。

流程图如下：

![usr1]({{ '/assets/images/conditiongenekp_1.png' | relative_url }}){: width=400px style="float:center"}

对于一对输入，$$\pmb{x}, \pmb{x}^{'}$$，使用$$\Phi(\pmb{x}^{'})$$获得$$K$$个heatmap，得到$$K$$个最大值点的位置，构造$$K$$个Gaussians，沿着channel维度连接起来，得到$$\pmb{y}^{'} \in \mathbb{R}_{+}^{H_o \times W_o \times K}$$，再和图片$$\pmb{x}$$的feature沿着channel维度连接起来，输入给decoder $$\Psi$$得到reconstruction的结果：$$\overline{\pmb{x}}^{'} = \Psi(\pmb{x}, \pmb{y}^{'})$$，而损失函数即为$$\overline{\pmb{x}}^{'}$$和$$\pmb{x}^{'}$$之间的MSE。

> 本方法的思想其实和AutoLink的也是有相似之处的，都是希望keypoints有足够的structure信息能够让decoder可以reconstruct回原图片，但AutoLink引入了edge map来表示keypoints的structure信息，更加fancy


#### \[**CVPR 2019**\] [D2-Net: A Trainable CNN for Joint Description and Detection of Local Features](https://openaccess.thecvf.com/content_CVPR_2019/papers/Dusmanu_D2-Net_A_Trainable_CNN_for_Joint_Description_and_Detection_of_CVPR_2019_paper.pdf)

*Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torri, Torsten Sattler*


在images之间建立像素点的对应关系是一个很基础的CV问题，其可以被应用在3D CV，视频压缩，跟踪，定位等等任务里。稀疏的局部特征是预估对应关系的一个重要方法。这些方法基于一种detect-then-describe的模式，先利用一个feature detector来找到一批keypoints，然后根据这些keypoints和其周围的点构建一些image patches，再利用descriptor对这些image patches给出feature description，作为这些keypoints的features。稀疏的局部特征方法有以下几个优势：1）对应关系可以通过nearest neighbor search和欧式距离很快的被找到。2）而且稀疏的特征所消耗的内存小，使得方法能够在大规模问题上应用。3）而且基于这种方法的keypoint detector一般都会考虑low-level的图像信息，比如说corners。从而局部特征可以被精确的在图像中定位，这对于很多后续任务比如说3D reconstruction来说是很重要的。

稀疏局部特征方法在很多图像条件下都得到了很好的使用。但是他们在极端的图像appearance变化的情况下就不好使了，比如说白天和夜里，或者季节变化，或者很弱纹理的场景。研究表明稀疏局部特征方法在这些情况下不好使的原因是keypoint detector检测到的keypoint的repeatability很差：因为按照上述描述的detector-then-describe流程，local descriptor会考虑keypoint和周围点的一块较大范围内的信息，从而潜在的会encode更high-level的结构，但keypoint detector只会考虑一个keypoint点的信息，范围太小。从而，这些keypoints的detections在appearance变化很大的时候就不稳定了。这是因为low-level信息会被图像里的low-level statistics影响更大，比如说像素点的intensity。

在这篇文章里，作者旨在同时做好这两件事情，也就是，找到在复杂条件下也能有很好效果的稀疏的特征集合，而且设计高效的匹配算法。为了达到这个目标，作者提出了一个describe-and-detect的方法来进行稀疏局部特征detection和description：和之前先利用low-level信息进行feature detection的方法不同，这个方法将detection stage放在了后面。该方法首先利用CNN计算feature maps，再利用这些feature maps来计算descriptors（在feature maps的每个像素位置，所有通道构成的向量为这个像素点的feature description）和检测keypoints（在feature maps上找到局部最大值）。通过这样操作，feature detector和feature descriptor就紧密相连了。由detector检测到的keypoints就会是那些匹配效果很好的descriptors了。同时，使用CNN深层输出的feature maps使得我们能够基于high-level的信息来计算feature detection和feature description。实验表明这种方法和dense方法相比所需要的内存要小得多，同时其也比之前的detect-then-describe方法对于复杂条件下的images的匹配效果要好得多，和dense方法效果差不多甚至更好。

和传统的使用了两个stages的pipeline的detect-then-describe方法不一样，这篇文章提出使用dense feature extraction来获得一个同时是detector和descriptor的representation。因为detector和descriptor使用了同样的representation，文章将这个方法叫做D2。方法流程如fig 1所示。

![performance]({{ '/assets/images/D2_1.png' | relative_url }}){: width=400px style="float:center"}
*Fig 1. detect-and-describe (D2) network。用一个feature extraction CNN $$\mathcal F$$来获取图片的feature map，这个feature map起到两个作用：(i) 在每个位置$$(i,j)$$的所有通道的值即为这个点的local descriptor $$d_{ij}$$；(ii) keypoints通过在feature map上使用一个non-local-maximum suppression，再在每个descriptor间使用一个non-maximum suppression来获得。keypoint detection score $$s_{ij}$$是通过一个soft local maximum score $$\alpha$$和一个每个descriptor的ratio-to-maximum score $$\beta$$计算得来的*

先利用某个预训练的网络来获取一个dense的feature map $$F = \mathcal F (I) \in \mathbb{R}^{h \times w \times n}$$，也叫做descriptors。利用每个像素点的descriptors之间的欧式距离可以很容易的计算两张图片之间的像素对应关系。在训练阶段，这些descriptors被训练为不同图片对应的同样的点具有相似的descriptors。每个descriptors被除以了它的$$L_2$$ norm来归一化这个descriptor。

而我们同时也可以将$$F$$看成一系列2D responses $$D$$的集合：$$D^k = F_{::k}, D^k \in \mathbb{R}^{h \times w}, \quad k=1,\cdots,n$$。在这个解释里，features $$\mathcal F$$可以被看成$$n$$个不同的2D的feature map，$$D^k$$，而它们则包含着keypoints的位置信息。这篇文章并不是简单的使用每个$$D^k$$获取一个keypoint，而是更复杂的设计。

对于使用传统的feature map，比如说DoG，来获取keypoints，我们会将该feature map通过一个空间的non-local-maximum suppression来稀疏化。但是在这篇文章的设定下，有多个feature maps $$D^k, k=1,\cdots,n$$，在他们中的任何一个上面都可以进行detection。因此，对于一个点$$(i,j)$$，如果它想要成为一个keypoint，则要满足：

$$(i,j) \quad \text{is} \  \text{a} \  \text{keypoint} \quad \iff D_{ij}^k \  \text{is} \  \text{a} \  \text{local} \  \text{max} \  \text{in} \ D^k, \quad \text{and} \ k = \arg\max\limits_{t} D{ij}^t$$

对于每个像素点$$(i,j)$$，上述设置会让我们在所有的detector $$D^k, k=1,2,\cdots,K$$里找到最显著的detector $$D^{k^{\ast}}$$（channel selection），然后再验证其在$$D^{k^{\ast}}$$上是否是一个局部最大值点。

在实践中，上述的hard detection procedure会被softened来满足back propagation。首先我们定义一个soft local-max score：

$$\alpha_{ij}^k = \frac{exp(D_{ij}^k)}{\sum_{(i^{'},j^{'}) \in \mathcal N(i,j)} exp(D_{i^{'}j^{'}}^k)}$$

其中$$\mathcal N(i,j)$$是像素点$$(i,j)$$周围包括自己一共9个点构成的集合。

然后我们再来定义soft channel selection，对于每个descriptor计算一个ratio-to-max来模仿每个通道的non-maximum suppression：

$$\beta_{ij}^k = D_{ij}^k / \max\limits_{t} D_{ij}^t$$

为了将上述两个criteria都考虑进来，我们对于所有的feature maps来最大化上述两个scores的积来获得一个简单的score map：

$$\gamma_{ij} = \max\limits_{k} (\alpha_{ij}^k \beta_{ij}^k)$$

最后，点$$(ij)$$的soft detection score $$s_{ij}$$通过一个image-level的归一化来得到：

$$s_{ij} = \gamma_{ij} / \sum_{(i^{'}, j^{'})} \gamma_{i^{'}j^{'}}$$

该方法只需要输入图片对之间ground truth的dense correspondence信息作为标签，并不需要keypoints的标签。



#### \[**ICCV 2019**\] [Joint Learning of Semantic Alignment and Object Landmark Detection](https://openaccess.thecvf.com/content_ICCV_2019/html/Jeon_Joint_Learning_of_Semantic_Alignment_and_Object_Landmark_Detection_ICCV_2019_paper.html)

![usr1]({{ '/assets/images/joint_1.png' | relative_url }}){: width=400px style="float:center"}

和[D2-Net: A Trainable CNN for Joint Description and Detection of Local Features](https://openaccess.thecvf.com/content_CVPR_2019/papers/Dusmanu_D2-Net_A_Trainable_CNN_for_Joint_Description_and_Detection_of_CVPR_2019_paper.pdf)
一样，本文将semantic correpondence学习和keypoint detection结合起来，输入是成对的图片，标签也只包含成对图片之间dense的correspondence，作者认为semantic correspondence的学习和keypoint detection这两个任务可以互相辅助。但因为没有keypoint的annotations，所以为了regularize由heatmaps得到的keypoints，作者还引入了$$\mathcal{L}_{\text{concentration}}$$和$$\mathcal{L}_{\text{separation}}$$来辅助学习。

> 注意，这篇文章从heatmaps提取keypoints的方式和其他的论文有所不同，对于网络输出的$$K+1$$个score map（还有个background的），注意score maps没有任何约束，可以取任何值。先对于每个像素点，沿着channel维度，计算softmax，得到$$K+1$$个heatmaps。然后对于每个表示keypoint的heatmap $$H_k$$，该heatmap获得的keypoint的计算方式为：$$\sum_{i=1}^H \sum_{j=1}^W (i,j) \times H_k(i,j) / \sum_{i=1}^H \sum_{j=1}^W H_k(i,j)$$。和别的方法的不同点在于，其并没有对于每个heatmap，在两个size维度进行softmax，而是在channel维度进行softmax。这样做的好处是可以使得检测到的keypoints天然的不易重复位置，但坏处是因为并没有沿着size维度进行softmax，可能会导致每个heatmap表示keypoint的位置不明显，影响精度。


#### \[**NeurIPS 2019**\] [Object landmark discovery through unsupervised adaptation](https://proceedings.neurips.cc/paper/2019/hash/97c99dd2a042908aabc0bafc64ddc028-Abstract.html)

[CODE](https://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019)

这篇文章的想法是，如果已经有了一个在某一个category上使用keypoint标注使用supervised的方法训练好的的keypoint detector（比如说就是之前的Hourglass network），那么对于新的category，是否可以保持该网络结构不变，只略微改变网络参数，就可以做到对这个新的category的图片的keypoint的detection。

其具体做法是对于Hourglass network来说（其包含pooling层、activation层、convolution层），只改变convolution层的参数。对于每一层，假设之前的网络参数是$$\Theta_{\mathcal{X}} \in \mathbb{R}^{C_o \times C_{in} \times k \times k}$$，其中$$C_o$$和$$C_{in}$$分别是输入输出channel维度，$$k$$是kernel size，那么我们对于该层只需要学习一个projection matrix $$W \in \mathbb{R}^{C_o \times C_o}$$，那么新的网络参数$$\Theta_{\mathcal{Y}}$$就可以表示为$$\Theta_{\mathcal{Y}} = W \Theta_{\mathcal{X}}$$，因为其维度和$$\Theta_{\mathcal{X}}$$相同，就可以直接使用原网络。

![usr1]({{ '/assets/images/adaptationkp_1.png' | relative_url }}){: width=400px style="float:center"}

而网络训练的损失函数，则和[Unsupervised Learning of Object Landmarks through Conditional Image Generation](https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html)相同，对于每张输入$$y$$，获取其deformed之后的图片$$y^{'}$$，从而得到输入图片对$$\left{ y, y^{'} \right}$$。


#### \[**NeurIPS 2020** $$\&$$ **TPAMI 2023**\] [Unsupervised Learning of Object Landmarks via Self-Training Correspondence](https://proceedings.neurips.cc/paper/2020/hash/32508f53f24c46f685870a075eaaa29c-Abstract.html)

[NeurIPS Version CODE](https://github.com/malldimi1/UnsupervisedLandmarks), [TPAMI Version CODE](https://github.com/malldimi1/KeypointsToLandmarks)

作者对于之前的unsupervised 2D keypoint inference方法的总结写的不错：

现有的无监督学习方法经常依赖于某些代理任务，而学习landmarks这样的一个主要任务通常是通过某种latent过程来实现的（比如说[Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning]）。还有一些方法使用某些代理任务，比如说equivariance：[Unsupervised learning of landmarks by descriptor vector exchange]，[Unsupervised learning of object frames by dense equivalent image labelling]，[Unsupervised learning of object landmarks by factorized spatial embeddings]，或者image generation：[Unsupervised learning of object landmarks through conditional image generation]，[Object landmark discovery through unsupervised adaption]，[Unsupervised discovery of object landmarks as structural representataions]。基于equivariance原则的方法表明，一个detector必须在经过已知的某种自定义的image deformation下保持连续（也就是说一张图片在经过image deformation前后，这个detector所检测到的keypoints要保证correspondence，其中这个image deformation是已知的），从而他们就利用这个原则来学习模型。而基于image generation的方法的流程是使用一个generator来reconstruct原输入图片，这个generator的输入是原图片经过了deformation之后的图片，而且这个generator是依赖于某个detector的输出的。detector和generator通过一个bottleneck来交换信息可以从输入里蒸馏出物体的几何信息，因为一个generator如果想从根据一张图片经过deformation之后的图片作为输入还能够reconstruct原输入照片，那它就需要对图片里物体的几何特征以及landmarks有足够的理解（也就是说，输入图片经过了某种deformation之后传给了generator，然后希望generator能够恢复原输入，这个时候generator所拿到的图片里的object的几何特征变了，但纹理特征没有，也就是说它需要找到deformation前后landmarks的对应关系来学习到这个deformation对于几何特征造成了什么样的影响， 才能够reconstruct原输入）。 

> 尽管这些方法在某些场景下取得了好的效果，但这些场景下物体一般都只有比较小的角度变化（比如说正面的人脸，人的身体，猫脸等），这些方法在以下两个角度上具有局限性。首先，代理任务并不能确保detector就能够显式的学习到物体的landmarks，因此会产生那些不太会被人类标注员所标注的landmarks。其次，这些方法都需要人为生成deformations，因为只有这样才能够产生一对图片，其landmarks之间的对应关系是知道的（deformation前后）。但是通过这样一对图片来学习（一张图片以及经过了deformation的这张图片）会导致模型对于更复杂的类内variation不具有鲁棒性，比如说背景变化、角度变化（经过了3D rotation）、或者说那些articulated的物体（比如说人的身体）。

> 在这篇文章里，作者观察到，尽管consistent的semantic的keypoints detector在无监督的方式下很难被训练，但从图片里获取一系列无序的没有correspondence的keypoints是很容易的（可以直接用Sobel filters（比如SIFT）或者预训练好的模型直接得到（比如SuperPoint）得到）。基于这个事实，作者提出了一个新的方法来从inconsistent的keypoints candidate set来获取consistent的keyponts。

作者发现，和先前的方法相比，本文提出的方法能够解决物体在进行很大角度变换之后的landmarks的对应关系，而且能解决对称的问题（也就是对称的物体的两个对称的landmark也是不同的），这一点从下图就能看出来。

![unsuper1]({{ '/assets/images/selftraining_1.png' | relative_url }}){: width=400px style="float:center"}

这篇文章对于无监督学习物体关键点提出了一种新的范式。和已有的利用image generation、equivariance或者autoencoder来进行无监督学习不同，本文提出的是一种自学习的方式，从general的keypoints candidates集合出发，来逐渐筛选并学习keypoints的detector和descriptor，逐渐将keypoints调整到更精确的位置上，将调整之后的点叫做landmarks。

> 为了避免误解，下面将不in correspondence的检测到的关键点叫做keypoints，将in correspondence的关键点叫做landmarks

具体来说，本文所提出的方法包括两个module：keypoint detector和keypoint descriptor。

对于keypoint detector来说，和之前的方法不同，作者在这里使用单个heatmap来预测所有的keypoints，也就是对于输入图片$$\text{I} \in \mathbb{R}^{H \times W \times 3}$$，输出heatmap为$$\text{H} \in \left( 0,1 \right)^{H_o \times W_o \times 1}$$，其中$$H_o,W_o$$和$$H,W$$等比例。而从heatmap获取keypoints的时候，使用的是非极大抑制（non-maximum suppression）的方法，具体来说就是，先对于设定的threshold，选出那些heatmap值大于threshold的positions。然后以每个position为中心，画出正方形方框（边长是一个hyperparameter），而该方框的score就是该position的heatmap的值，最后再结合一个IoU threshold（也是一个hyperparameter），使用`torchvision.ops.batched_nms`来获得最终的filter之后的点的集合（filter掉那些挨得太近的），这样对于每张图片得到的keypoint的数量并不是确定的，而且也并不in correspondence，将所有图片的所有检测到的这样的keypoints记为：$$\lbrace I_j, \lbrace p_i^j \rbrace_{i=1}^{N_k} \rbrace$$，其中$$p_i^j \in \mathbb{R}^2$$是一个keypoint二维坐标，$$N_j$$是图片$$I_j$$里所检测到的keypoints的数量。

对于keypoint descriptor来说，先对于输入图片，学习到一个dense的descriptor（也就是每个pixel都有个descriptor），$$F \in \mathbb{R}^{H \times W \times D}$$，然后根据上一步keypoint detector所得到的每张图片的keypoints集合，将每张图片每个keypoint对应pixel位置的descriptor取出来，将所有图片的所有按这样方式得到的features记为：$$\lbrace I_j, \lbrace f_i^j \rbrace_{i=1}^{N_k} \rbrace$$，其中$$f_i^j \in \mathbb{R}^D$$是一个keypoint二维坐标，$$N_j$$是图片$$I_j$$里所检测到的keypoints的数量。然后就到了关键步骤，对于所有的这些features，进行两次K-means的计算。具体来说，第一次K-means，将中心的数量设置为$$K$$（一个hyperparameter），而且对于每张图片来说，对于每个聚类中心，只保留一个keypoint（也就是说，如果对于某张图片，有两个该张图片检测到的keypoint都被分为到某个聚类中心$$C_k$$上，那么只保留距离该聚类中心最近的那个），因为只有这样才make sense（对于每个semantic的landmark，每张图片只应该检测到一个）。按照聚类的结果，我们可以filter掉一批keypoints（因为每张图片每个聚类中心只能有一个keypoint，也就是说现在每张图片至多保留$$K$$个keypoints）。第二次K-means，将中心的数量设置为$$M$$（也是一个hyperparameter，但是要远大于$$K$$，作者说这样设置的好处是可以account for因为大的角度变化所导致的features的不稳定，也正是因为这个原因，才能使得对于有大角度变化的数据集，检测到的landmarks仍然in correspondence），然后再次运行K-means算法，同样对于每个聚类中心，每张图片至多有一个keypoint能被标注为该聚类中心，从而现在的labels就有$$M$$类。

在训练的算法里，对于每个iterations来说，先使用keypoint detector获得每张图片的一个keypoints的集合，再使用keypoint descriptor得到对应的features的集合，再对于所有的训练数据（或者某一固定的部分）来作上述的clustering，从而对于keypoints进行filter，而且每张图片的每个keypoint都有了一个label（属于哪个聚类中心）。而损失函数包含两个部分：（1）每张图片filter之后的keypoints构成的联合Gaussian heatmap和keypoint detector输出的heatmap之间的差别，以及（2）使用聚类得到每张图片每个keypoint的label之后，基于keypoint features的contrastive loss（两张图片的同一label的keypoints的features相近，同一张图片的不同label的keypoints的features相远）。由该loss来对keypoint detector以及keypoint descriptor一起训练。

![unsuper2]({{ '/assets/images/selftraining_2.png' | relative_url }}){: width=400px style="float:center"}
*Fig 2. 上述所描述方法的流程图。所用到的网络是具有共用的backbone的两个输出头的网络，一个头是detector head，另一个是descriptor head。在训练过程中，算法将会在利用descriptor的features进行clustering从而给keypoints打上伪标签，与利用这种伪标签进行自学习，这两个过程之间反复横跳。和之前的那些方法都不同，本文所使用的方法并不需要一张图片以及它的deformation作为输入图片对，这种输入会减弱模型学习类内variation的能力。correspondence recovery是利用文中提出的修改版K-means实现的。本文的方法还可以用来恢复丢失的landmarks。上图还显示了使用t-SNE来可视化数据集里的keypoint对应位置的features的结果*

> 作者表示，需要将$$M$$设置为远大于$$K$$的值：这样会导致feature space的过分割问题，但这个却可以使得每个landmark可以对应好几个clusters，这样带来的好处就是它可以解决图片中物体视角变化非常大的情况，因为由于视角变化，实际上这两张图片里的同样区域的clusters的features不太一样，但它们都被贴在了同一个landmark上，可以由后续操作将它们合并。这一点也是本文的方法和之前的工作的一个不同，本文的方法可以解决角度变化很大的情况。

> 因为该算法流程是在keypoint detection和利用keypoint descriptor和clustering进行labelling这两个module之间iterative进行的，所以就需要一个起始值。这篇文章使用SuperPoint来对每张图片获取初始的keypoint candidates set。而且其还对keypoint descriptor的初始化进行了一些warm up的处理。具体来说，最一开始，在$$t=0$$的时刻，训练集$$X_0$$仅仅包含$$\lbrace x_j, \lbrace p_i^j \rbrace_{i=1}^{N_j} \rbrace$$，而并没有keypoint之间的correspondence $$f_i^j$$。在NeurIPS的版本里，最初始的features是由某个预训练好的网络给的（SuperPoint），然后就可以进行后续的循环了。但在TPAMI的版本里，作者采用了一个warm up的预训练stage，也就是只利用一张图片和它的deformation构成的图片对作为输入来训练网络。这样的话keypoints之间的对应关系就已经知道了，这就是positive pairs。也就是利用equivariance性质来初始化backbone和feature extractor head。

在上述流程循环了足够多次之后，我们所得到的结果是对于每张图片，都可以得到一个keypoint set，而且这些keypoints也都有labels（$$M$$个类），以及keypoints对应的descriptors。但我们的终极目标是，对于每张图片，输出固定数量（$$K$$个）landmark坐标。

所以，将上述算法称为stage1，而我们还需要在stage1结束之后，增加一个stage2。

现在keypoints的labels数量远大于$$K$$（$$M$$个），但我们并不知道到底哪些clusters需要对应到同一个landmark。在NeurIPS那个版本里，这个过程是通过逐渐合并clusters来减少clusters数量到$$K$$个来实现的。然而，因为现在每张图片的keypoints数量最多是$$K$$个，也因为采用了更好的negative pair的选取策略，作者发现即使有$$M$$个clusters，这些clusters也自动形成了$$K$$个分离的很好的clusters。这个事实就使得我们并不再需要在NeurIPS里所使用的逐渐合并clusters的这样一个流程，只需要再次使用了具有$$K$$个聚类中心的K-means就可以了。之后就可以根据由stage1得到的标注以及这里对标注从$$M$$个到$$K$$个的合并，重新训练一个和supervised的方法一样的输出$$K$$个heatmap的keypoint detector了，这就是stage2。

#### \[**CVPR 2020 Oral**\] [Self-supervised Learning of Interpretable Keypoints from Unlabelled Videos](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/data/unsupervised_pose.pdf)

[POST](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/)

这篇文章的效果非常好（更多的图可以去项目主页看到）：

![unsuper1]({{ '/assets/images/unlabelvideokp_1.png' | relative_url }}){: width=400px style="float:center"}
*Fig 1. 从上图可以看到，即使是左右翻转的情况，keypoints的对应关系仍然是正确的，即并没有将左手识别为右手，右手识别为左手*

效果这么好的原因有两个：（1）使用了skeleton来表示structure，而不仅仅是keypoints；（2）使用了未配对的ground truth的keypoint和skeleton标注

这篇文章仍然是基于[Unsupervised Learning of Object Landmarks through Conditional Image Generation](https://proceedings.neurips.cc/paper/2018/hash/1f36c15d6a3d18d52e8d493bc8187cb9-Abstract.html)，即输入仍然是一对描述同一个物体的不同图片，$$\lbrace \pmb{x}, \pmb{x}^{'} \rbrace$$（在这里就是一个视频的两帧），并且由detector $$\Phi$$来对输入$$\pmb{x}$$预测其的某种structure representation $$\Phi(\pmb{x})$$，然后和$$\pmb{x}^{'}$$一起作为decoder $$\Psi$$的输入，来reconstruct $$\pmb{x}$$。在之前的文章里，该structure representation是由Gaussian的heatmaps堆叠而成的，但在这篇文章里，其是skeleton，记为$$p$$。具体来说，$$p = \Phi(\pmb{x})$$。但没有任何限制的话，$$p$$可以是任意的形式，所以其设计为：$$p = \beta(\eta(\Phi(\pmb{x})))$$，其中$$\eta$$是将skeleton映射为keypoint matrix $$M \in \mathbb{R}^{K \times 2}$$的函数（由neural network实现），而$$\beta$$是将keypoint matrix $$M$$映射为skeleton $$p \in \mathbb{R}^{H \times W}$$的函数。

$$\beta$$的具体设计为，对于某个预先设定好的limbs set $$E$$（$$E$$里是keypoints pairs）、某个temperature hyperparameter $$\gamma$$，$$H \times W$$里的任意一个位置$$u$$的值$$p(u)$$为：

$$p(u) = exp(-\gamma \min\limits_{(i,j) \in E, r \in \left[0, 1 \right]} \lVert u - rp_i - (1-r)p_j \rVert_2^2)$$

直观来说，也就是以$$E$$里那些limbs构成的联合高斯分布。

本文的视觉效果那么好的第二个原因是它们直接使用了标注好的keypoints和skeleton的ground truth。但其并没有对图片直接进行标注，这样的标注是和数据集内任何一张图片都不匹配的，也就是说有$$M$$个这样的$$p$$构成的集合：$$\lbrace p_j \rbrace_{j=1}^M$$。

> 实际上也就是给了shape template

作者说之所以skeleton比keypoint构成的Gaussian heatmap的效果要好，是因为让$$\Phi(\pmb{x})$$输出skeleton而不是输出离散的keypoint matrix这样一个任务类似于image-to-image translation（skeleton可以理解为一张灰度图），而这个任务更加适合神经网络去学习。

而且为了让$$\Phi(\pmb{x})$$能更好的输出skeleton，以及为了利用那$$M$$个ground truth，作者引入了一个discriminator $$D$$，将$$\Phi$$当作generator，来生成尽可能和那$$M$$个ground truth相似的skeleton。

![unsuper2]({{ '/assets/images/unlabelvideokp_2.png' | relative_url }}){: width=400px style="float:center"}



#### \[**NeurIPS 2022 Spotlight**\] [AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints](https://arxiv.org/pdf/2205.10636.pdf)

[CODE](https://github.com/xingzhehe/AutoLink-Self-supervised-Learning-of-Human-Skeletons-and-Object-Outlines-by-Linking-Keypoints)

这篇文章要解决的问题是无监督的从一类物体的图片里学习到2D keypoints。模型的输入是一类物体（比如人脸）的RGB图片，输出是2D keypoints，数量和顺序是固定的。也就是说，模型在输入图片后，输出$$K$$个heatmap，然后从$$K$$个heatmap里获取$$K$$个2D keypoints。所以不同图片的2D keypoints之间的correspondence也是直接就有的。

很多无监督2D keypoints的方法都是一个auto encoder的结构，但这篇文章的创新点有如下几个：

* 首先，网络除了输出$$K$$个heatmap之外，还会输出一个$$K \times K$$的graph，用来表示没两对keypoints之间的权重，而且这个graph是global的，也就说对于每个输入图片，其是共同的。
* 其次，对于每对输出的keypoints，构建一个edge heatmap，大小和输入图片一样，是以这两个keypoint的连线为中心的Gaussian分布，再利用上面的graph，得到每个像素点位置的global edge heatmap的值
* 在得到这个global edge heatmap之后，还是一样需要一个decoder来reconstruct原输入图片，显然只有这个骨架是没办法还原的，所以文章的做法是对于输入图片，mask掉其绝大部分区域，和这个edge heatmap沿着channel连起来，作为decoder的输入。

> 实际上AutoLink基于auto encoder结构，且思路类似于[Self-supervised Learning of Interpretable Keypoints from Unlabelled Videos](https://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/data/unsupervised_pose.pdf)，但AutoLink将整个框架变成了一个unsupervised的流程，这是其的主要贡献

这篇文章的方法很简单，却很有效果。

有个要注意的技术细节是，reconstruction loss不仅仅是reconstructed的图片和原图片之间的mse loss，而是perception loss，也就是将这两个图片都输入某个预训练好的网络，比如在ImageNet上预训练好的VGG19，然后对比很多层的输出之间的差异的和。这样做要比仅仅在像素层面比较区别更加鲁棒。


#### \[**CVPR 2024 Highlight**\] [Unsupervised Keypoints from Pretrained Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Hedlin_Unsupervised_Keypoints_from_Pretrained_Diffusion_Models_CVPR_2024_paper.pdf)

*Eric Hedlin, Gopal Sharma, Shweta Mahajan, Xingzhe He, Hossam Isack, Abhishek Kar, Helge Rhodin, Andrea Tagliasacchi, Kwang Moo Yi*

[CODE](https://github.com/ubc-vision/StableKeypoints)

这篇文章和[Unsupervised Semantic Correspondence Using Stable Diffusion](https://ubc-vision.github.io/LDM_correspondences/)一样，基于的想法是，对于训练好的text-based StableDiffusion模型，即使是random的text embedding，和multi-head encoder得到的image embedding计算相似度再可视化，都能看到图中object大致的形状，并且相似度高的区域具有一定的semantics。

所以本文的想法是，对于预训练好的StableDiffusion，optimize若干个text embedding，使得每个text embedding和image embedding计算完相似度之后（这个相似度matrix就可以理解成一个heatmap），该heatmap是单峰的，这也就意味着该text embedding对应着所有图片的某一固定区域（比如眼睛），如下图所示：

![unsuper1]({{ '/assets/images/stablekeypoints_1.png' | relative_url }}){: width=800px style="float:center"} 

具体来说，假设transformer有$$L$$层，每一层有$$C$$个head，那么每一层就可以得到$$C$$个pixel-level的image embedding：$$F_{I}^c \in \mathbb{R}^{H \times W \times D}$$，其中$$c=1,2,\cdots,C$$，$$H,W$$是图片尺寸，$$D$$是特征维度。对于text-embedding来说，假设有$$N$$个token，这样所有的token的embedding集合是$$F_{T} \in \mathbb{R}^{N \times D}$$。从而就可以计算pixel-level的text和image pixel的相似度：$$M_c \mathbb{R}_{+}^{H \times W \times N}$$。最后的相似度矩阵$$M$$是先将$$M_c$$沿着$$N$$维度作softmax，再对于所有的head进行一个average，最后再在$$L$$层上选出几层出来average。

在得到了相似度矩阵$$M \mathbb{R}_{+}^{H \times W \times N}$$之后，其loss是对于每一个$$M_i \mathbb{R}_{+}^{H \times W \times N}$$，$$i=1,2,\cdots,N$$，先找到该$$M_i$$最大值所在的坐标，以该坐标为中心，在$$H \times W$$的2维grid上构建一个高斯分布（方差是超参数），然后计算$$M_i$$与该高斯分布之间的损失。

有几点值得注意的技术细节：
* 首先，在训练的时候，除了上述的损失函数，还加上了equivariance损失，输入图片对是通过手动制作transformation实现的
* 其次，因为keypoints存在遮挡的情况，所以对于某些图片来说，可能计算出来的相似矩阵是比较弥散的（即不能对应一个尖峰高斯，因为其本来就检测不到）。为了解决这个问题，作者提出的engineering的方法是先设定一个较大的keypoint值，比如说50，最后选择那些和高斯分布的KL散度最小的相似矩阵对应的那些keypoint作为输出，比如说25
* 最后，为了实现所得到的keypoints确实能够覆盖object，作者还在上一步所得到的那些keypoint里，继续使用fartest point sampling选择出一个subset作为最终的输出，比如说15

和autolink相比，本文的方法对于pose比较大的情况，确实能够正确识别，比如下图：

![unsuper1]({{ '/assets/images/stablekeypoints_2.png' | relative_url }}){: width=800px style="float:center"} 

但是对于某些情况，比如说人体，该方法仍然不能正确的区分正反面（可能是因为人脸这种用于区分正反面的特征在图片里的区域太小了，导致特征信息不够）：

![unsuper1]({{ '/assets/images/stablekeypoints_3.png' | relative_url }}){: width=800px style="float:center"} 

> 而且本文的另一个缺点是，其是针对整个数据集的所有图片找到它们**最公共**的$$K$$个keypoints输出，这样对于某些数据集，比如CUB_200_2011来说，其绝大多数图片都是鸟的两个侧面之一，所以最终输出的$$K$$个keypoints都是侧面的这些点，但对于偶尔出现的鸟的俯视或者正视图，这$$K$$个keypoints里的有些点在图中都没有ground truth的对应

> 而且实际上，StableKeypoints的textual tokens还是表示的是local信息，因为从CUB_200_2011或者是Human3.6m的结果来看，其并不能分清左右，即左手和右手，或者左眼和右眼，它们的features是一样的，对应同一个textual token

## 3D Keypoint Detection

### Supervised 3D Keypoint Detection

#### \[**NeurIPS 2018 Oral**\] [Occlusion-Net: 2D/3D Occluded Keypoint Localization Using Graph Networks](https://openaccess.thecvf.com/content_CVPR_2019/papers/Reddy_Occlusion-Net_2D3D_Occluded_Keypoint_Localization_Using_Graph_Networks_CVPR_2019_paper.pdf)

[CODE](https://github.com/dineshreddy91/Occlusion_Net)

![unsuper1]({{ '/assets/images/occlusion_1.png' | relative_url }}){: width=800px style="float:center"} 

这篇文章的输入是RGB图片，以及固定的$$K$$个keypoint标注，以及这$$K$$个keypoint的visibility的情况（0或者1），也就是说，本文的输入setting和C3DPO等NrSfM方法的setting是一样的。而本文的思路是，使用Graph Neural Network来对所有的$$K$$个keypoint以及它们之间的edges进行建模，从而可以用每张图片的visible keypoints来预测invisible keypoints。keypoints之间的edges就是0或者1（如果两个keypoints在该张图片都可见，就是1，其他情况是0）。

本文使用Mask-RCNN输出$$K$$个heatmap来表示$$K$$个keypoints，同时也使用heatmap来表示keypoints的confidence，再加上visibility，共同作为GNN的每个node的输入features。

> 本文的实验是在一个car dataset上做的，该dataset背景复杂，且可能含有多个cars，但car本身是rigid objects，所以该数据集有它的难度，但在rigidity上变化较小。

![unsuper1]({{ '/assets/images/occlusion_2.png' | relative_url }}){: width=800px style="float:center"} 

#### \[**CVPR 2023**\] [Few-shot Geometry-Aware Keypoint Localization](https://xingzhehe.github.io/FewShot3DKP/)



#### \[**CVPR 2024**\] [3D-LFM: Lifting Foundation Model](https://3dlfm.github.io/)


### Non-rigid Structure from Motion (NrSfM)

#### \[**ICCV 2019**\] [C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion](https://openaccess.thecvf.com/content_ICCV_2019/papers/Novotny_C3DPO_Canonical_3D_Pose_Networks_for_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf)

[[CODE](https://github.com/facebookresearch/c3dpo_nrsfm)]

这篇文章解决的问题是NrSfM，输入是同一个物体的不同角度的views的2D keypoints annotations，也就是一个$$2 \times K$$的矩阵和一个$$1 \times K$$的0，1矩阵用来表示keypoints的visibility，$$K$$是超参数，keypoints的数量，输出是该物体的3D keypoints，也就是3D shape，大小为$$3 \times K$$。输入并不是RGB图片。

> 注意，visibility矩阵非常重要，其在两个地方都有作用：1）其是作为factorization network $$\phi$$的输入的。具体来说，网络首先将keypoint矩阵和visibility矩阵相乘（从而invisible的那些列就是0了），再将乘积后的矩阵和visibility矩阵align，得到$$3 \times K$$的矩阵输入给$$\phi$$得到feature，再进行后续的shape coefficient和viewpoint的预测。2）在reprojection loss里，只计算那些可见keypoint的projected 2d keypoints和gt keypoints之间的差异。

这篇文章的亮点在于，作为2019年的文章，还处于使用deep learning解决NrSfM问题的中期，在现在来看方法并不复杂，但好的效果和好的可视化整体来看是不错的。文章的主要想法是要将由物体的rigid motion transformation导致的2D keypoints不同，与物体形变（比如人体视频不同帧因为动作变化导致的物体形变）导致的不同区分开。文章是通过引入两个loss来解决这个问题。

第一个loss很显然，网络在以2D keypoints matrix作为输出后，并不是直接输出3D keypoint matrix，而是输出一个basis $$S \in \mathbb{R}^{3D \times K}$$，一个依赖于输入的coefficient vector $$\alpha \in \mathbb{R}^{1 \times D}$$，然后将3D matrix用这个basis的线性组合来表示：$$X = (\alpha \bigotimes I_3)S$$，其中$$\bigotimes$$是Kronecker product，而$$D$$是超参数。再然后，将这个3D matrix $$X$$ 经过rotation $$R$$之后再project到2D上，和输入的2D ground truth进行比较，计算loss。

上述有几个技术性细节：
* rotation matrix $$R$$和coefficient $$\alpha$$均为网络的输出，basis $$S$$是网络参数
* 在预处理数据的时候就将2D keypoint的x和y维度分别进行了zero-center处理，并且整体乘上了一个scalar，使得variance较大的那个轴（x或y）的数值范围大概位于-1到1之间。这样的话，就可以在计算transformation的时候不用考虑translation了。
* 在计算loss的时候，计算的是两个matrix或者vector之间的loss，用的不是一般的loss，而是humber loss，暂时还不知道为什么要这样。
* 输入不仅仅有$$Y$$，实际上还有每个keypoint是否visible的flag vector $$v$$，在计算loss的时候，这些$$v$$就乘以每个keypoints，也就是说，不可见的就不算在内。

第二个loss是作者为了使得网络认为所有的只经过rigid body transformation后的shape都应该等价所加上的。具体做法是，再设计一个网络$$\Psi$$，前一个网络叫做$$\Phi$$，对于任何一个2D keypoint matrix $$Y$$输入，$$\Phi$$输出了$$\alpha$$和$$R$$，以及$$S$$，从而计算出了3D matrix $$X$$。对于网络$$\Psi$$，先随机采样一个rotation matrix $$R^{'}$$，然后将$$R^{'}X$$输入$$\Psi$$，输出$$\alpha^{'}$$（注意，并不是直接输出3D shape）。然后，结合$$S$$，得到了一个新的3D shape $$X^{'} = (\alpha^{'} \bigotimes I_3) S$$，新的loss就是$$X^{'}$$和$$X$$之间的距离。

最后还有个可有可无的loss，也就是还可以在plane内加上rotation，也就是说不是对于$$X$$加上3D rotation matrix，而是直接对于$$Y$$加上2D rotation matrix，这是用来使得网络$$\Phi$$更robust的，可以理解为一种data augmentation。

流程图如下：


![C3DPO-1]({{ '/assets/images/C3DPO-1.png' | relative_url }}){: width=800 style="float:center"} 


> 注意，网络并不是直接输出的rotation matrix，而是输出了一个长度为3的向量，然后经过hat operator和matrix exponential计算，得到了rotation matrix。参考[hat operator](https://en.wikipedia.org/wiki/Hat_operator)，[matrix exponential](https://en.wikipedia.org/wiki/Matrix_exponential)



#### \[**ICCV 2019**\] [Deep Non-Rigid Structure from Motion](https://openaccess.thecvf.com/content_ICCV_2019/papers/Kong_Deep_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf)


#### \[**3DV 2020**\] [Deep NRSfM++: Towards Unsupervised 2D-3D Lifting in the Wild](https://arxiv.org/pdf/2001.10090)


#### \[**ECCV 2020**\] [Procrustean Regression Networks: Learning 3D Structure of Non-Rigid Objects from 2D Annotations](https://arxiv.org/pdf/2007.10961.pdf)

[[CODE](https://github.com/sungheonpark/PRN)]

**line of research**

[C3DPO](https://openaccess.thecvf.com/content_ICCV_2019/papers/Novotny_C3DPO_Canonical_3D_Pose_Networks_for_Non-Rigid_Structure_From_Motion_ICCV_2019_paper.pdf) $$\longrightarrow$$ [PRN](https://arxiv.org/pdf/2007.10961.pdf)

[EM-PND](https://openaccess.thecvf.com/content_cvpr_2013/papers/Lee_Procrustean_Normal_Distribution_2013_CVPR_paper.pdf) $$\longrightarrow$$ [PR](https://ieeexplore.ieee.org/document/8052164) $$\longrightarrow$$ [PRN](https://arxiv.org/pdf/2007.10961.pdf)

其中，PRN是基于C3DPO的想法，也想要将rigid motion和object本身形变造成的2D keypoints不同给分离开，C3DPO专门设计了一个新的网络$$\Psi$$来解决这个，而PRN则是通过将每个shape都用Generalized procrustes analysis align到一起叠成一个matrix，然后对这个matrix进行约束实现的（比如说进行nuclear norm约束其rank等）。而PRN的这个使用GPA进行align的想法则是源于PR这篇论文，而PR的想法则是源于EM-PND。EM-PND是希望使用procrustean distribution来表示这些aligned的shapes，而使用EM算法来对参数进行学习。

所以说，PRN的核心就是使用某种在aligned shapes上的regularization term来替代C3DPO里的canonicalization网络，而这些aligned shapes则是通过GPA计算得来。在PRN里，procrustean analysis的reference shape是aligned的shape的平均值，而在PR里这也是个可学习的参数。

**技术细节**

* 对于任意一个3D shape $$X_i \in \mathbb{R}^{3 \times n_p}$$，和reference shape，$$\bar{X}$$，aligned所使用的rotation matrix是这样计算得来的：$$R_i = \mathop{argmin}\limits_{R} \lVert RX_iT - \bar{X} \rVert$$，其中$$R_i^T R = I$$，$$T = I_{n_p} - \frac{1}{n_p} 1_{n_p} 1_{n_p}^T$$是translation matrix，用于将shape $$X_i$$center到origin上。这里的$$T$$的用法可以被借鉴。而aligned的shape就是$$\tilde{X_i} = R_i X_i T$$。
* PRN和PR这两篇文章都花了大量的篇幅证明上述网络设计的每个部分都是differentiable的（计算出来了loss对于$$X_i$$和reference shape $$\bar{X}$$的导数），所以说GPA也可以被放在可学习的框架内。
* PRN相对于C3DPO还有个创新就是，其的输入既可以是和C3DPO一样，是2D keypoint matrix，也可以是RGB图片，分别使用MLP和CNN来作为网络框架。


#### \[**CVPR 2021**\] [PAUL: Procrustean Autoencoder for Unsupervised Lifting](https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PAUL_Procrustean_Autoencoder_for_Unsupervised_Lifting_CVPR_2021_paper.pdf)

#### \[**WACV 2024**\] [Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling](https://openaccess.thecvf.com/content/WACV2024/papers/Ji_Unsupervised_3D_Pose_Estimation_With_Non-Rigid_Structure-From-Motion_Modeling_WACV_2024_paper.pdf)

#### \[**CVPR 2024**\] [Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment and Spatially-variant Deformation Modeling](https://npucvr.github.io/TSM-NRSfM/)




### Unsupervised 3D Keypoint Detection

#### \[**ECCV 2018**\] [Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency](https://github.com/xingyizhou/3DKeypoints-DA)

[CODE](https://github.com/xingyizhou/3DKeypoints-DA)

这篇文章的想法和那篇unsupervised 2D keypoint detection的论文[Object landmark discovery through unsupervised adaptation](https://proceedings.neurips.cc/paper/2019/hash/97c99dd2a042908aabc0bafc64ddc028-Abstract.html)类似，都是让网络在某个类别的source datasets上supervised训练，然后将这个网络参数微调，使得其能够适应新的另一个类别的target dataset。但这里的数据是3D keypoints。

这篇文章的数据集包含了丰富的信息。数据集构成为：一个source datasets，每个数据点都有RGB图片$$\pmb{x}$$以及对应的3D keypoint matrix $$P \in \mathbb{R}^{K \times 3}$$；一个target datasets，其包含$$N$$个场景，而每个场景又包含$$M_j$$个multi views，$$j=1,2,\cdots, N$$。

具体做法是，使用一个神经网络$$\Phi$$来使用图片作为输入，输出3D keypoint matrix，损失函数包括三个：（1）在source datasets上预测的3D keypoints和ground truth之间的差异；（2）在target dataset上，对于每个场景，同时也optimize一个$$M_i \in \mathbb{R}^{K \times 3}$$，代表这个场景在canonical frame下的3D keypoints的坐标，然后对于这个场景的每个view $$I_j, j=1,2,\cdots,M_j$$，先得到3D keypoints的输出$$X_{I_j} = \Phi(I_j) \in \mathbb{R}^{K \times 3}$$，然后使用procrustes analysis将$$X_{I_j}\ $$align到$$M_i$$上去，计算aligned之后的$$X_{I_j}$$和$$M_i$$之间的距离，即为multi-view loss；（3）最后一个loss用来减小网络对于source dataset和target dataset输出之间的差异，具体做法是，对于每个$$M_i$$，其对source datasets里的每个ground truth $$Y_j$$都做procrustes analysis，然后计算aligned后的$$M_i$$和这些$$Y_j$$之间的距离，选择最小的那个作为该$$M_i$$和source datasets之间的距离，然后对所有的$$M_i$$做上述操作，将距离求和，即为chamfer loss。


#### \[**NeurIPS 2018 Oral**\] [Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning](https://keypointnet.github.io/keypointnet_neurips.pdf)
*Supasorn Suwajanakorn, Noah Snavely, Jonathan Tompson, Mohammad Norouzi*

[Review](https://papers.nips.cc/paper/2018/file/24146db4eb48c718b84cae0a0799dcfc-Reviews.html)
[CODE](https://github.com/ZhengyiLuo/KeypointNet)

3D pose estimation的标准做法是：1）先检测一个稀疏的category-specific keypoints的集合；2）之后再利用这些keypoints使用某种geometric reasoning框架（比如说，PnP algorithm）来重建3D pose或者相机角度。我们可以利用很强的监督信息，对于每个object category，利用很多张标有keypoints位置的该类object的图片，来学习一个针对该类object的keypoint detector。或者使用model-based fitting方法来学习这样的keypoint detector。研究者已经建立了标有keypoints的face，hands，human body的datasets。但是，对于某个object category的图片里的keypoints进行选取和标注的过程是十分复杂而且ill-defined的。

为了设计合理的keypoint set，需要将这个keypoint set要用在什么下游任务这个信息考虑进来。直接利用下游任务来优化keypoints的选择会自然的鼓励网络学到对于这个下游任务最有用的那些keypoints。这篇文章提出了KeypointNet，一个end-to-end的geometric reasoning框架，来对于一个特别的下游任务，学习一个最优的category-specific 3D keypoints。我们的方法相对于先前工作的创新点在于我们利用一个任意的下游任务来将keypoints作为latent variable学习出来。我们的框架对于任何目标函数对于keypoints positions是可微分的下游任务都是可以使用的。KeypointNet将keypoints作为下游任务的latent variables来优化。具体来说，本文使用了pose estimation作为下游任务，即目标函数旨在为恢复同一个object的两个views之间的relative pose。

![PIPELINE]({{ '/assets/images/DOWNSTREAM-2.PNG' | relative_url }}){: width=800px style="float:center"}
*Fig 2. 在训练过程中，同一个object的两个views都会给KeypointNet作为输入。两个views之间的rigid transformation $$(R,t)$$也作为监督信号被提供。我们学习一个3D keypoints的ordered list，在两个views里是consistent的而且能够让我们复原transformation。在inference的时候，KeypointNet从一张单独的输入图片提取3D keypoint positions。*


训练集的输入包括一对图片$$(I,I^{'})$$，其是同一个object的两个不同角度的图片，并且有它们的relative rigid transformation $$T \in SE(3)$$，将潜在的3D shapes从$$I$$变换到了$$I^{'}$$。我们将通过优化一个目标函数$$O(f_{\theta}(I), f_{\theta}(I^{'}))$$，来学习一个函数$$f_{\theta}(I)$$，参数是$$\theta$$，将一个2D的图片$$I$$映射到一个3D keypoint locations的list，$$P=(p_1,p_2,...,p_N)$$，其中$$p_i=(u_i,v_i,z_i)$$。

网络输出一个probability distribution map，$$g_i(u,v)$$，表示的是keypoint i在pixel位置$$(u,v)$$地方可能出现的概率，而$$\Sigma_{u,v}g_i(u,v)=1$$对于所有的$$i$$都成立。我们使用一个spatial softmax来对整个image pixels生成这样的distribution。我们之后再计算这个spatial distribution的期望来获得一个pixel coordinate：

$$\left[u_i, v_i\right]^T = \Sigma_{u,v} \left[ug_i(u,v), vg_i(u,v)\right]^T$$

对于$$z$$ coordinates来说，我们同样再每个pixel的位置预测一个深度值，叫做$$d_i(u,v)$$，然后：

$$z_i = \Sigma_{u,v} d_i(u,v) g_i(u,v)$$


我们的multi-view consistency loss的目的就是确保keypoints能在不同的views里跟踪consistent的部分。两个views里的3D keypoints应该分别在3D空间里对应到object的同一个3D像素点。我们假设已知相机的global focal length $$f$$。下面我们使用$$\left[x,y,z\right]$$代表world coordinate坐标，使用$$\left[u,v\right]$$代表image coordinate坐标（使用perspective projection）

定义一个symmetric multi-view consistency loss如下：

$$L_{con} = \frac{1}{2N}\Sigma_{i=1}^N ||\left[u_i, v_i, u_i^{'}, v_i^{'} \right]^T - \left[\hat u_{i}^{'}, \hat v_{i}^{'}, \hat u_{i}, \hat v_{i} \right]^T||^2$$

定义一个可微分的目标函数用来衡量预测的relative rotation $$\hat{R}$$（用两个keypoints集合利用Procrustes'alignment计算得来）和ground truth rotation $$R$$之间的差异。因为我们的KeypointNet具有translation equivariance性质，并且我们已经有了上述的multi-view consistency loss，我们在这个loss里就不衡量translation loss了。从而这个pose estimation目标函数定义为：

$$ L_{pose} = 2arcsin(\frac{1}{2\sqrt{2}}||\hat{R}-R||)$$

这衡量了从两个keypoints集合里计算出的least-squares estimate $$\hat{R}$$和ground truth relative rotation matrix $$R$$之间的角度差异。

为了估计$$\hat{R}$$，$$X$$和$$X^{'} \in R^{3 \times N}$$表示两个views的camera coordinates的3D keypoint locations.也就是说，假设$$X = \left[X_1,X_2,...,X_N \right]$$以及$$X_i = (\pi^{-1}pi)\left[:3 \right]$$。$$X^{'}$$也有类似的定义。$$\tilde{X}$$和$$\tilde{X}^{'}$$表示$$X$$和$$X^{'}$$分别将自身的mean减掉之后的矩阵。从而optimal least-squares rotation $$\hat{R}$$就用以下的方式计算而得：

$$\hat{R} = Vdiag(1,1,...,det(VU^T))U^T$$

其中$$U, \Sigma, V^T = SVD(\tilde X \tilde X^{'T})$$。这个计算rotation $$\hat{R}$$的方式叫做orthogonal Procrustes problem。为了确保$$\tilde{X}\tilde{X}^{'T}$$是invertible的，并且为了增强keypoints的鲁棒性，我们在$$X$$和$$X^{'}$$内加入Gaussian噪音。


> 对于很多下游的任务来说，还有很多常见的keypoints的性质可以起到帮助的作用，比如说：1）没有两个keypoints能对应到同一个3D location；2）keypoints需要在一个object的轮廓内。Separation loss对于两个靠的太近的keypoints进行惩罚：$$L_{sep} = \frac{1}{N^2}\Sigma_{i=1}^{N}\Sigma_{j \neq i}^N max(0, \delta^2-||X_i- X_j||^2)$$/。和consistency loss不一样，这个loss是3D loss，也就是说即使两个pixel占据了同一个像素点，但只要它们的深度z不同，那么它们就可以这样分布。
> Silhouette consistency鼓励keypoints落在object的轮廓内。我们网络对于第$$i$$个keypoint的position的预测是通过计算$$g_i$$（也就是heatmap）的期望得来的。一个确保silhouette consistency的方法就是只在silhouette内让概率非零，设计一个单峰的low variance的spatial distribution。图片的mask $$b(u,v) \in \{0,1\}$$，其中1表示前景object，0表示背景。从而silhouette consistency loss可以被定义为：$$L_{obj} = \frac{1}{N}\Sigma_{i=1}^N -log\Sigma_{u,v}b(u,v)g_i(u,v)$$
> 还有个loss用来使得variance比较小：$$L_{var} = \frac{1}{N}\Sigma_{i=1}^N \Sigma_{u,v} g_i(u,v)||\left[u,v\right] - \left[u_i, v_i\right]||^2$$，这一项使得distribution峰比较尖，从而帮助控制keypoint落在object的边界内。



#### \[**ICML 2021**\] [Unsupervised Learning of Visual 3D Keypoints for Control](http://proceedings.mlr.press/v139/chen21b/chen21b.pdf)

*Boyuan Chen, Pieter Abbeel, Deepak Pathak*

[Code](https://github.com/buoyancy99/unsup-3d-keypoints) [Post](https://buoyancy99.github.io/unsup-3d-keypoints/)


这篇文章使用了一种非监督的方式，设计了一个end-to-end的模型，直接从2D图片里学习到3D keypoints。其是通过一个multi-view consistency约束以及一个下游任务来训练网络的。为了让模型具有普适性并且效果好，其需要满足以下三个性质：(a) 在3D空间里的consistency，也就是说从同一个场景的不同的scenes里所学习到的3D keypoints在3D空间里应该位于同一个位置。(b) 在时间上consistency：同一个keypoint在时间上也应该是连续的 (c) Joint learning with control：因为我们是为了control任务来找的keypoints，联合训练可以使得我们找到的keypoints更加有效。

给定某个camera view的一张image，我们首先预测在image space里的keypoints locations和depth。对于这些从不同的camera views获得的这些keypoints，利用一个differentiable unprojection操作来获得每个keypoint的world coordinate。通过multi-view consistency loss来学习到的不同view之间的consistency可以使得不同view的keypoints可以映射到同一个world coordinate上。这个world coordinate再投射到每个camera view对应的image plane上来重构输入的原image。这样的设计构造了一个differentiable 3D keypoint bottleneck。我们的keypoint学习和RL任务是同时被优化的。整个过程由fig 1所示。我们的模型叫做Keypoint3D。

我们使用了一个multi-view encoder-decoder的结构不使用监督信息来学习3D keypoints。给定同一个场景的$$N$$个view，我们为每个view都给一个encoder和一个decoder。我们为学习到3D keypoints提供了三个unsupervised的signal：1) 我们通过让不同view所学习到的keypoints都能映射到同一个3D空间内的3D keypoint来迫使所学习到的keypoints具有geometrically consistent的特性。2) 我们利用reconstruction loss来惩罚decoder的不准确的reconstruction。3) 我们利用RL任务的reward来反向传播到encoder里从而训练encoder的参数。

$$I_n \in R^{H \times W \times C}$$表示相机$$n$$的输入image，$$n \in 1,\cdots, N$$，而相机$$n$$具有extrinsic matrix $$V_n$$，和intrinsic matrix $$P_n$$。$$K$$是我们keypoints的个数。对于一个3D空间里的点$$\left[x,y,z\right]^T$$和camera $$n$$，我们可以使用extrinsic matrix $$V_n$$和perspective intrinsic matrix $$P_n$$来将其投射到camera coordinate $$\left[u, v, d\right]^T$$，其中$$u,v \in \left[0,1\right]$$是camera plane上归一化后的coordinate，$$d>0$$是depth value，也就是那个点距离camera plane的距离。operator $$\Omega_n: \left[x,y,z\right]^T \longrightarrow \left[u,v,d\right]^T$$表示上述的这种投射，而其的inverse记为$$\Omega_n^{-1}$$。$$\Omega_n, \Omega_n^{-1}$$都是differentiable的，而且可以被解析表示。


**Step1 Keypoint Encoder**

对于每个camera $$n$$，我们将$$I_n$$喂给一个fully convolutional encoder $$\phi_n$$来获得$$k$$个confidence maps，$$C_n^k \in R^{S \times S}$$，以及depth maps，$$D_n^k \in R^{S \times S}$$。对于每个confidence map，我们使用一个spatial softmax来计算一个probability heatmap $$H_n^k$$：

$$H_n^k(i,j) = \frac{exp(C_n^k(i,j)}{\Sigma_{p=1}^S \Sigma_{q=1}^{S} exp(C_n^k(p,q))}$$

heatmap $$H_n^k \in R^{S \times S}$$里的每个值表示的是一个3D keypoint $$k$$出现在从camera $$n$$的角度产生的2D image plane上的这个点的概率。而depth map $$D_n^k$$表示的是在每个camera plane的位置，这个3D keypoint距离camera plane的距离。

然后我们就可以计算keypoint $$k$$在camera $$n$$的camera plane下的坐标了：

$$E\left[u_n^k\right] = \frac{1}{S} \Sigma_{u,v} u H_n^k(u,v)$$

$$E\left[v_n^k\right] = \frac{1}{S} \Sigma_{u,v} v H_n^k(u,v)$$

$$E\left[d_n^k\right] = \Sigma_{u,v} u D_n^k(u,v) H_n^k(u,v)$$

注意到，对于$$u,v$$的计算都除以了$$S$$，也就是图片尺寸，是因为正如我们之前提到的，我们计算的是归一化之后的相机坐标下的keypoint的坐标。

记$$\left[\hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T = \left[E\left[u_n^k\right], E\left[v_n^k\right], E\left[d_n^k\right]\right]^T$$。

为了进一步增加我们的方法的可靠性，我们并不直接将上述的encoder的结果作为keypoint的camera plane的坐标值，而是利用一个高斯分布，其均值为这个值，方差为1，在整个image平面上随机选取，进一步增加了模型的可靠性。


**Step2 Attention**

在预测了每个camera coordinate frame下每个keypoints的坐标之后，我们要想办法将每个keypoint的$$n$$个不同camera下的坐标统一起来。一个最简单的方法就是取平均。但是在某些角度下的keypoints可能被遮挡，从而预测效果并不好。为了解决这个问题，我们利用之前的confidence maps来设计一个加权平均。这使得我们对于那些不那么自信的view里获得的keypoint的权重要小一些，从而不影响整体的效果。

我们可以为每个camera $$n$$获取的keypoint $$k$$设置一个confidence score，其和confidence map $$C_n^k$$的平均值成比例，而且对于$$K$$个keypoints，还做了归一化处理：

$$A_n^k = \frac{exp(\frac{1}{S^2}\Sigma_{p=1}^S \Sigma_{q=1}^S C_n^k(p,q))}{\Sigma_{i=1}^K exp(\frac{1}{S^2}\Sigma_{p=1}^S \Sigma_{q=1}^S C_n^i(p,q))}$$

这个就可以被理解为，对于camera $$n$$来说，其对于每个估计到的keypoint分配的概率，这$$K$$个keypoints的概率总和为1。


**Step3 Extracting world coordinates**

给定之前由encoder预测到的camera plane内的keypoints $$\left[ \hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T$$，$$n= 1,\cdots, N$$，$$k=1, \cdots, K$$，我们可以将其反投射回world coordinates：$$\left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]^T = \Omega_n^{-1}(\left[ \hat u_n^k, \hat v_n^k, \hat d_n^k \right]^T)$$。这个就是从camera $$n$$获取到的keypoint $$k$$的world coordinate。对于每个keypoint，我们都有$$N$$个预测的结果，我们利用之前计算的$$A_n^k$$来为每个keypoint计算一个加权的world coordinate：

$$\left[\bar x^k, \bar y^k, \bar z^k \right]^T = \Sigma_{n=1}^N \frac{A_n^k}{\Sigma_{m=1}^N A_n^m} \left[ \hat x_n^k, \hat y_n^k, \hat z_n^k \right]^T$$

**Step4 Keypoint Decoder**

我们在decoder之前，还需要将$$K$$个keypoints都再投射回camera plane来增强模型的学习能力。对于每个camera $$n$$和每个keypoint，我们有$$\left[\bar u, \bar v, \bar d \right]^T = \Omega_n(\left[\bar x^k, \bar y^k, \bar z^k \right]^T)$$。为了获取空间结构信息，对于每个camera和每个keypoint，我们都构建一个高斯分布$$G_n^k \in R^{S \times S}$$，均值为$$\left[\bar u, \bar v \right]$$，方差为$$I_2 / \bar d$$。这个分布使得离得近的那些keypoint，在camera plane上具有更分散的分布。

我们记$$\bar A^k = \frac{1}{N} \Sigma_{n=1}^N A_n^k$$为对于所有view的平均attention，每个camera的decoder $$\psi_n$$就将stacked的高斯maps $$G_n$$作为输入来重构输入image $$I_n$$，而$$G_n = K stack(\left[G_n^1 \bar A^1, \cdots, G_n^K \bar A^K \right])$$。


![Model Structure]({{ '/assets/images/CONTROL-1.PNG' | relative_url }})
{: style="width: 800px; max-width: 100%;"}
*Fig 1. Overview of our Keypoint3D algorithm. (a) 对于每个camera view，一个CNN将输入image编码为$$K$$个heatmaps以及depth maps；(b) 我们将这些heatmaps当作概率来计算camera plane下keypoint横纵坐标的。我们同时也用heatmap和depth map来计算每个keypoint的深度d，这些$$\left[u,v,d\right]$$再被反投射回到world coordinate里；(c) 我们利用之前的heatmaps来计算每个camera view对于每个keypoint的置信概率，然后对于每个keypoint，我们计算出一个加权的world coordinate；(d) 我们再将每个keypoint的world coordinate投射到每个camera plane上；(e) 从而对于每个camera plane，都有$$K$$个这样的投射，建立$$K$$个高斯map，将它们叠起来，作为decoder的输入，来重构原输入图片；(f) 除了上述的这些loss，我们还将所学习到的3D keypoint的world coordinate与下游任务相结合，来共同优化这个网络。*

#### \[**CVPR 2022**\] [Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects](https://openaccess.thecvf.com/content/CVPR2022/html/Noguchi_Watch_It_Move_Unsupervised_Discovery_of_3D_Joints_for_Re-Posing_CVPR_2022_paper.html)
